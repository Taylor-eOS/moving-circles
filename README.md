## moving-circles - robot desk car experiment
This program implements a tiny online supervised learning agent inside a grid-world robot simulator. At each time step the robot converts local proximity readings into three numeric inputs, passes them through a single logistic unit that produces a scalar “danger” probability, uses a simple rule to decide motion, and then immediately treats the environment outcome as a training label and updates the unit’s parameters.<br>
Sensors and inputs. The robot senses distance in three directions relative to its heading: front, left, right. Each sensor scans up to SENSOR_RANGE cells and returns a normalized proximity value equal to (SENSOR_RANGE − dist + 1)/SENSOR_RANGE when an obstacle is at distance dist (so an obstacle immediately adjacent yields 1.0, farther obstacles give fractional values, and no obstacle returns 0.0). Those three floats are the only inputs to the learning unit.<br>
The model. The learner is a single linear combination followed by a sigmoid nonlinearity. Computation is score = b + w_front*front + w_left*left + w_right*right, prediction = sigmoid(score). The sigmoid implementation clips its input to the interval [−10,10] to avoid overflow and then applies the usual logistic formula 1/(1+e^(−x)). The prediction is interpreted as the probability that moving forward is “dangerous.”<br>
Label generation. The program generates its own training label each step by checking the grid cell in front of the robot and a short look-ahead (it tests the immediate next cell and two cells beyond). If any of those cells contains an obstacle the label actual_danger = 1.0, otherwise 0.0. In short, the environment supplies a binary danger signal based on imminent collisions.<br>
Learning rule. After computing prediction and label, the code computes error = prediction − actual_danger and adjusts parameters with simple gradient-like steps:<br>
w ← clamp(w − α * error * input, −5, 5), b ← clamp(b − α * error, −5, 5), with α = LEARNING_RATE. This is an online update performed every time step (no batches, no separate training phase). Note that because the update omits multiplying by the derivative of the sigmoid, it is not the true gradient of the squared or cross-entropy loss; it is a simpler delta-style update that will still move predictions toward the labels but does not follow exact logistic gradient descent.<br>
Decision policy and closed loop. The predicted danger probability directly influences action. If prediction > 0.5 the agent executes an evasive turn (randomly left or right). If prediction ≤ 0.5 the agent mostly attempts to move forward, with a small random turning probability (15%) to add exploration. Movement is blocked by obstacles; if a forward move is blocked the robot also randomly turns. Because learning and decision are tightly coupled, the agent’s actions change the sensory distribution it learns from, producing a closed-loop, on-policy training process.<br>
Diagnostics and constraints. The program prints sensor values, predicted danger, actual label, recent accuracy (a rolling fraction over the last 50 steps of whether prediction>0.5 matched label>0.5), and the three weights plus bias. Weights and bias are clamped to [−5,5] so parameters cannot explode. The architecture is extremely simple: three inputs, one linear layer, one sigmoid output. Computational cost is trivial: a few arithmetic ops per step.<br>
Modeling consequences and limitations. Because the model is linear before the sigmoid it can only learn a single planar decision boundary in the three-dimensional sensor space. The simplistic learning rule ignores the sigmoid derivative and therefore may converge slower or behave differently from true logistic regression. The online self-labeling scheme means there is no held-out test distribution and the agent’s behavior influences the data it gets, so biases and failure modes can persist. Sensor range and the label look-ahead do not exactly match (sensors look out to SENSOR_RANGE, label checks two cells ahead), which can create systematic mismatches the learner must compensate for by adjusting weights and bias. There is no explicit noise model, no regularization beyond clamping, and no memory of states besides the recent accuracy buffer.<br>
In computational terms the project is a minimal demonstration of online, embodied supervised learning: convert continuous sensor inputs to a logistic output, use an environment-derived binary label, apply an immediate delta update to weights, and act using the prediction while logging simple performance statistics. The code illustrates how a simple linear classifier can be trained in closed loop inside a reactive robotic policy, and it also exposes the typical pitfalls of such tight coupling between learning and control.
